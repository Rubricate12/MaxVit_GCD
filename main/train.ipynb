{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bryan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import timm\n",
    "import torch\n",
    "import zipfile,os\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_zip = '/DATA/ai lab/gcdData/GCD.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/DATA/ai lab')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/DATA/ai lab/GCD/train\n"
     ]
    }
   ],
   "source": [
    "train_dir = '/DATA/ai lab/GCD/train'\n",
    "test_dir = '/DATA/ai lab/GCD/test'\n",
    "\n",
    "os.listdir('/DATA/ai lab/GCD/train')\n",
    "print(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\n",
    "    'maxvit_tiny_tf_224.in1k',\n",
    "    pretrained=True,\n",
    "    num_classes=7,\n",
    ")\n",
    "model = model.eval()\n",
    "\n",
    "# get model specific transforms (normalization, resize)\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "trans = timm.data.create_transform(**data_config, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_dir = Path(train_dir)\\n\\nfor img_path in train_dir.rglob(\"*.*\"):  # Recursively find all files\\n    if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:  # Check for valid image extensions\\n        img = Image.open(img_path).convert(\"RGB\")  # Open the image and convert to RGB\\n        img_tensor = transforms(img).unsqueeze(0)  # Preprocess and add batch dimension\\n\\n        # Pass the image through the model\\n        output = model.forward_features(transforms(img_tensor))\\n\\n        print(f\"Processed {img_path}: Output Shape - {output[-1].shape if isinstance(output, list) else output.shape}\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_dir = Path(train_dir)\n",
    "\n",
    "for img_path in train_dir.rglob(\"*.*\"):  # Recursively find all files\n",
    "    if img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]:  # Check for valid image extensions\n",
    "        img = Image.open(img_path).convert(\"RGB\")  # Open the image and convert to RGB\n",
    "        img_tensor = transforms(img).unsqueeze(0)  # Preprocess and add batch dimension\n",
    "\n",
    "        # Pass the image through the model\n",
    "        output = model.forward_features(transforms(img_tensor))\n",
    "\n",
    "        print(f\"Processed {img_path}: Output Shape - {output[-1].shape if isinstance(output, list) else output.shape}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Compose' object has no attribute 'Compose'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 29\u001b[0m\n\u001b[0;32m     25\u001b[0m             image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(image)\n\u001b[0;32m     27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m image, label  \u001b[38;5;66;03m# Ensure this returns a tuple of (image, label)\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m transform \u001b[38;5;241m=\u001b[39m \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompose\u001b[49m([\n\u001b[0;32m     30\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),  \u001b[38;5;66;03m# Resize images to 224x224\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),           \u001b[38;5;66;03m# Convert images to PyTorch tensors\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])  \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[0;32m     33\u001b[0m ])\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Create an instance of the CustomDataset\u001b[39;00m\n\u001b[0;32m     36\u001b[0m dataset \u001b[38;5;241m=\u001b[39m CustomDataset(data_dir\u001b[38;5;241m=\u001b[39mtrain_dir, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Compose' object has no attribute 'Compose'"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_names = os.listdir(data_dir)\n",
    "\n",
    "        for label, class_name in enumerate(self.class_names):\n",
    "            class_dir = os.path.join(data_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                self.images.append(img_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.images[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label  # Ensure this returns a tuple of (image, label)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "])\n",
    "\n",
    "# Create an instance of the CustomDataset\n",
    "dataset = CustomDataset(data_dir=train_dir, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "for data in train_loader:\n",
    "    print(data[0].shape)  # This will show you the structure of the data being returned\n",
    "    inputs, targets = data  # Unpack only if it has the correct structure\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Define loss function and optimizer\u001b[39;00m\n\u001b[0;32m      6\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m----> 7\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(\u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m      9\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 20\n",
    "# Training loop\n",
    "model.train()  # Set the model to training mode\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)  # Move to device\n",
    "\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, targets)  # Compute loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# After training, you can evaluate the model as you did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "test_dataset = ImageFolder(root=test_dir, transform=transforms)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 4. Define testing loop\n",
    "correct_predictions = 0\n",
    "total_samples = 0\n",
    "all_targets = []\n",
    "all_preds = []\n",
    "\n",
    "'''\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for inputs, targets in test_loader:\n",
    "        # Perform forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Get predicted class indices\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        \n",
    "        # Compare predictions with ground truth\n",
    "        correct_predictions += (preds == targets).sum().item()\n",
    "        total_samples += targets.size(0)\n",
    "        \n",
    "        # Store predictions and labels for metrics\n",
    "        all_targets.extend(targets.tolist())\n",
    "        all_preds.extend(preds.tolist())\n",
    "'''\n",
    "# Perform forward pass\n",
    "outputs = model(inputs)\n",
    "        \n",
    "# Get predicted class indices\n",
    "_, preds = torch.max(outputs, dim=1)\n",
    "        \n",
    "# Compare predictions with ground truth\n",
    "correct_predictions += (preds == targets).sum().item()\n",
    "total_samples += targets.size(0)\n",
    "        \n",
    "# Store predictions and labels for metrics\n",
    "all_targets.extend(targets.tolist())\n",
    "all_preds.extend(preds.tolist())        \n",
    "\n",
    "accuracy = accuracy_score(all_targets, all_preds)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
